exp 8

data <- c(4, 7, 8, 9, 7, 10, 5, 7, 6, 8)
mean_value <- mean(data)
cat("Mean:", mean_value, "\n")
median_value <- median(data)
cat("Median:", median_value, "\n")
mode_value <- as.numeric(names(sort(table(data), decreasing = TRUE)[1]))
cat("Mode:", mode_value, "\n")
----------------------------------------------------------------
8B
input <- mtcars[,c("am","mpg","hp")]
print(head(input))
input <- mtcars
result <- aov(mpg~hp*am,data = input)
print(summary(result))
result <- aov(mpg~hp+am,data = input)
print(summary(result))
result1 <- aov(mpg~hp*am,data = input)
result2 <- aov(mpg~hp+am,data = input)
print(anova(result1,result2))
------------------------------------------------------------------
exp9 linearregression
# Define the height and weight data
height <- c(150, 160, 170, 180, 190)
weight <- c(50, 60, 70, 80, 90)
model <- lm(weight ~ height)
plot(height, weight, main = "Height vs. Weight",
     xlab = "Height (cm)", ylab = "Weight (kg)", pch = 16)
abline(model, col = "red")
----------------------------------------------------------------------
exp9 logistic 
# Define the age and buying data
age <- c(20, 30, 40, 50, 60)
buy <- c(0, 1, 0, 1, 1) 
model <- glm(buy ~ age, family = binomial)
summary(model)
predicted_probs <- predict(model, newdata = data.frame(age = c(25, 35)), type = "response")
print(predicted_probs)
age_seq <- seq(min(age), max(age), length.out = 100)
fitted_probs <- predict(model, newdata = data.frame(age = age_seq), type = "response")
plot(age, buy, main = "Logistic Regression: Age vs. Buying",
     xlab = "Age", ylab = "Buying Probability", pch = 16, col = "blue", ylim = c(0, 1))
lines(age_seq, fitted_probs, col = "red", lwd = 2)
--------------------------------------------------------------------------
exp9 multiple
# Sample Data for Multiple Regression (Height, Age vs Weight)
set.seed(123)
age <- c(25, 30, 35, 40, 45)  
height_multi <- c(150, 160, 170, 180, 190) 
weight_multi <- c(50, 60, 65, 75, 85) 
multiple_model <- lm(weight_multi ~ height_multi + age)
summary(multiple_model)
library(ggplot2)
ggplot(data = data.frame(weight_multi, fitted = fitted(multiple_model)), aes(x = fitted, y = weight_multi)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  ggtitle("Multiple Regression: Residuals vs Fitted Values") +
  theme_minimal()
---------------------------------------------------------------
exp 9 posion

set.seed(123)
exercises <- rpois(5, lambda = exp(0.05 * height)) 
summary(poisson_model)
library(ggplot2)
ggplot(data = data.frame(height, exercises), aes(x = height, y = exercises)) +
  geom_point(aes(color = exercises)) +
  geom_smooth(method = "glm", method.args = list(family = "poisson"), color = "blue") +
  ggtitle("Poisson Regression: Exercises per Week vs Height") +
  theme_minimal()
-------------------------------------------------------------
exp10 time series
# Sample Data: Sales over 6 months
sales <- c(100, 120, 140, 160, 180, 200) 
time_series <- ts(sales, frequency = 12, start = c(2023, 1)) 
plot(time_series, 
     main = "Sales Time Series", 
     xlab = "Month", 
     ylab = "Sales", 
     col = "blue")
linear_model <- lm(sales ~ c(1:6)) 
abline(linear_model, col = "red") 
--------------------------------------------------------------------
exp10non lineaer

# Define the values

xvalues <- c(1.6, 2.1, 2, 2.23, 3.71, 3.25, 3.4, 3.86, 1.19, 2.21)
yvalues <- c(5.19, 7.43, 6.94, 8.11, 18.75, 14.88, 16.06, 19.12, 3.21, 7.58)
png(file = "nls.png")
plot(xvalues, yvalues)
model <- nls(yvalues ~ b1*xvalues^2 + b2, start = list(b1 = 1, b2 = 3))
lines(seq(min(xvalues), max(xvalues), length.out = 100), predict(model, newdata = data.frame(xvalues = seq(min(xvalues), max(xvalues), length.out = 100))))
dev.off()
print(sum(resid(model)^2))
print(confint(model))
------------------------------------------
exp10 decision tree

data("iris")
install.packages("caret")
install.packages("C50")
library(caret)
library(C50)
set.seed(7)
inTraininglocal <- createDataPartition(iris$Species, p = .70, list = FALSE)
training <- iris[inTraininglocal, ]
testing <- iris[-inTraininglocal, ]
model <- C5.0(Species ~ ., data = training)
summary(model)
pred <- predict(model, testing[, -5])  # use predict() instead of predict.C5.0()
a <- table(testing$Species, pred)
accuracy <- sum(diag(a)) / sum(a)
print(accuracy)
plot(model)
-------------------------------------------------------
exp11 normal distribution
->dnorm
x <- seq(-10, 10, by = .1)
y <- dnorm(x, mean = 2.5, sd = 0.5)
png(file = "dnorm.png")
plot(x,y)
dev.off()
plot(x,y)
->pnorm
x <- seq(-10,10,by = .2)
y <- pnorm(x, mean = 2.5, sd = 2)
png(file = "pnorm.png")
plot(x,y)
dev.off()
plot(x,y)
->qnorm
x <- seq(0, 1, by = 0.02)
y <- qnorm(x, mean = 2, sd = 1)
png(file = "qnorm.png")
plot(x,y)
dev.off()
plot(x,y)
->rnorm
y <- rnorm(50)
png(file = "rnorm.png")
hist(y, main = "Normal DIstribution")
dev.off()
hist(y,  main = "Normal Distribution")
------------------------------------------
exp 11 binomial
->dbinom

# Create a sample of 50 numbers which are incremented by 1.
x <- seq(0,50,by = 1)
# Create the binomial distribution.
y <- dbinom(x,50,0.5)
png(file = "dbinom.png")
plot(x,y)
dev.off()
plot(x,y)

->pbinom
# Probability of getting 26 or less heads from a 51 tosses of a coin.
x <- pbinom(26,51,0.5)
print(x)
->qbinom
# How many heads will have a probability of 0.25 will come out when a coin
# is tossed 51 times.
x <- qbinom(0.25,51,1/2)
print(x)
->rbinom
# Find 8 random values from a sample of 150 with probability of 0.4.
x <- rbinom(8,150,.4)
print(x)
-------------------------------------------------------------
exp12 chisqare test

observed <- matrix(c(20, 15, 15, 25, 15, 10), nrow = 2, byrow = TRUE)
rownames(observed) <- c("Male", "Female")
colnames(observed) <- c("Product A", "Product B", "Product C")
chi_square_test <- chisq.test(observed)
print(chi_square_test)
mosaicplot(observed, main = "Mosaic Plot of Gender vs Product Preference", color = TRUE)

->t-test

# Step 1: Create the test scores for both groups
group1 <- c(78, 85, 91, 88, 84, 79, 95)
group2 <- c(72, 74, 69, 75, 70, 68, 74)  
boxplot(group1, group2, names = c("Tutored", "Non-Tutored"), 
        main = "Test Scores Comparison", 
        ylab = "Test Scores", 
        col = c("lightblue", "lightgreen"))
t_test_result <- t.test(group1, group2, alternative = "two.sided", var.equal = TRUE)
print(t_test_result)


->f-test

# Step 1: Create the data for each department

department_A <- c(50, 52, 55, 49, 51)
department_B <- c(60, 62, 64, 58, 61)
department_C <- c(45, 47, 48, 43, 44)
department <- factor(c(rep("A", length(department_A)),
                       rep("B", length(department_B)),
                       rep("C", length(department_C))))
income <- c(department_A, department_B, department_C)
data <- data.frame(department, income)
anova_result <- aov(income ~ department, data = data)
summary(anova_result)
